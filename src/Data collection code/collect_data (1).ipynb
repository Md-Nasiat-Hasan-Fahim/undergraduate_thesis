{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8c0ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] Collecting halobacteria...\n",
      "Query: (taxonomy_id:183963) AND (fragment:false) AND (length:[100 TO 800]) AND NOT (keyword:methanogen)\n",
      "Rows saved: 1000000\n",
      "[DONE] Saved 1013662 rows to halobacteria.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# 1. User Identification (REQUIRED by UniProt to avoid 403 Errors)\n",
    "# Please replace with your actual email so UniProt can contact you if the script causes issues.\n",
    "CONTACT_EMAIL = \"uusshas12@gmail.com\"\n",
    "\n",
    "# 2. Define Datasets & Queries\n",
    "# Limits removed to collect ALL available data as requested.\n",
    "datasets = {\n",
    "    # \"thermoproteota\": {\n",
    "    #     \"query\": \"(taxonomy_id:28889) AND (fragment:false) AND (length:[100 TO 800])\",\n",
    "    #     \"limit\": None    # Download all (~222k)\n",
    "    # },\n",
    "    \"halobacteria\": {\n",
    "        \"query\": \"(taxonomy_id:183963) AND (fragment:false) AND (length:[100 TO 800]) AND NOT (keyword:methanogen)\",\n",
    "        \"limit\": None    # Download all (~1M)\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Define Columns (Mapped to UniProt API Fields)\n",
    "fields = [\n",
    "    \"id\",                       # Entry Name\n",
    "    \"gene_names\",               # Gene Names\n",
    "    \"organism_name\",            # Organism\n",
    "    \"organism_id\",              # Organism (ID)\n",
    "    \"protein_name\",             # Protein names\n",
    "    \"xref_proteomes\",           # Proteomes (Group by this later)\n",
    "    \"fragment\",                 # Fragment\n",
    "    \"length\",                   # Length\n",
    "    \"sequence\",                 # Sequence\n",
    "    \"absorption\",               # Absorption\n",
    "    \"ft_act_site\",              # Active site\n",
    "    \"ft_binding\",               # Binding site\n",
    "    \"cc_catalytic_activity\",    # Catalytic activity\n",
    "    \"cc_cofactor\",              # Cofactor\n",
    "    \"ft_dna_bind\",              # DNA binding\n",
    "    \"cc_activity_regulation\",   # Activity regulation\n",
    "    \"cc_function\",              # Function [CC]\n",
    "    \"kinetics\",                 # Kinetics\n",
    "    \"cc_pathway\",               # Pathway\n",
    "    \"ph_dependence\",            # pH dependence\n",
    "    \"ft_site\",                  # Site\n",
    "    \"temp_dependence\",          # Temperature dependence\n",
    "    \"reviewed\",                 # Reviewed\n",
    "    \"go_p\",                     # GO (biological process)\n",
    "    \"go_c\",                     # GO (cellular component)\n",
    "    \"go\",                       # Gene Ontology (GO)\n",
    "    \"go_f\",                     # GO (molecular function)\n",
    "    \"go_id\",                    # Gene Ontology IDs\n",
    "    \"ft_mutagen\",               # Mutagenesis (Lab mutations)\n",
    "    \"ft_variant\",               # Natural variant (Crucial for disease/evolution)\n",
    "    \"cc_subcellular_location\",  # Subcellular location [CC]\n",
    "    \"structure_3d\",             # 3D\n",
    "    \"protein_families\",         # Protein families\n",
    "    \"cc_similarity\",            # Sequence similarities\n",
    "    \"xref_alphafolddb\"          # AlphaFoldDB\n",
    "]\n",
    "\n",
    "API_URL = \"https://rest.uniprot.org/uniprotkb/stream\"\n",
    "\n",
    "# ==========================================\n",
    "# DOWNLOAD FUNCTION\n",
    "# ==========================================\n",
    "def download_data(name, config):\n",
    "    filename = f\"{name}.csv\"\n",
    "    print(f\"\\n[START] Collecting {name}...\")\n",
    "    print(f\"Query: {config['query']}\")\n",
    "    \n",
    "    # UniProt requires a User-Agent header with an email\n",
    "    headers = {\n",
    "        \"User-Agent\": f\"PythonScript/1.0 ({CONTACT_EMAIL})\"\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"query\": config[\"query\"],\n",
    "        \"format\": \"tsv\",     # TSV is safer to stream\n",
    "        \"fields\": \",\".join(fields)\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    try:\n",
    "        # stream=True is essential for large datasets\n",
    "        with requests.get(API_URL, params=params, headers=headers, stream=True, timeout=60) as response:\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # iter_lines yields byte strings, we decode them\n",
    "            lines = response.iter_lines(decode_unicode=True)\n",
    "            \n",
    "            with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                \n",
    "                # Write Header\n",
    "                try:\n",
    "                    header = next(lines).split(\"\\t\")\n",
    "                    writer.writerow(header)\n",
    "                except StopIteration:\n",
    "                    print(f\"[ERROR] No data found for {name}\")\n",
    "                    return\n",
    "\n",
    "                # Write Data\n",
    "                for line in lines:\n",
    "                    if line:\n",
    "                        writer.writerow(line.split(\"\\t\"))\n",
    "                        count += 1\n",
    "                        \n",
    "                        # Progress indicator (every 50k rows for speed)\n",
    "                        if count % 50000 == 0:\n",
    "                            sys.stdout.write(f\"\\rRows saved: {count}\")\n",
    "                            sys.stdout.flush()\n",
    "                        \n",
    "                        if config[\"limit\"] and count >= config[\"limit\"]:\n",
    "                            print(f\"\\n[STOP] Reached limit of {config['limit']}\")\n",
    "                            break\n",
    "        \n",
    "        print(f\"\\n[DONE] Saved {count} rows to {filename}\")\n",
    "        \n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"\\n[ERROR] HTTP Error: {err}\")\n",
    "        if response.status_code == 403:\n",
    "            print(\"Hint: 403 usually means UniProt blocked the script. Check your CONTACT_EMAIL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed to download {name}: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# RUN\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    for name, config in datasets.items():\n",
    "        download_data(name, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5517cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# 1. User Identification (REQUIRED by UniProt to avoid 403 Errors)\n",
    "# Please replace with your actual email so UniProt can contact you if the script causes issues.\n",
    "CONTACT_EMAIL = \"uusshas12@gmail.com\"\n",
    "\n",
    "# 2. Define Columns (Mapped to UniProt API Fields)\n",
    "fields = [\n",
    "    \"id\", \"gene_names\", \"organism_name\", \"organism_id\", \"protein_name\", \n",
    "    \"xref_proteomes\", \"fragment\", \"length\", \"sequence\", \"absorption\", \n",
    "    \"ft_act_site\", \"ft_binding\", \"cc_catalytic_activity\", \"cc_cofactor\", \n",
    "    \"ft_dna_bind\", \"cc_activity_regulation\", \"cc_function\", \"kinetics\", \n",
    "    \"cc_pathway\", \"ph_dependence\", \"ft_site\", \"temp_dependence\", \n",
    "    \"reviewed\", \"go_p\", \"go_c\", \"go\", \"go_f\", \"go_id\", \"ft_mutagen\", \n",
    "    \"ft_variant\", \"cc_subcellular_location\", \"structure_3d\", \n",
    "    \"protein_families\", \"cc_similarity\", \"xref_alphafolddb\"\n",
    "]\n",
    "\n",
    "API_URL = \"https://rest.uniprot.org/uniprotkb/stream\"\n",
    "\n",
    "# ==========================================\n",
    "# DOWNLOAD FUNCTION (Supports Write and Append)\n",
    "# ==========================================\n",
    "def download_data(name, query, filename, write_mode, write_header, limit=None):\n",
    "    \"\"\"\n",
    "    Downloads data from UniProt using the streaming API and writes to a file.\n",
    "    \n",
    "    :param name: A descriptive name for the current operation (e.g., 'Gammaproteobacteria 100-200').\n",
    "    :param query: The specific UniProt API query string.\n",
    "    :param filename: The file to write to (will be opened in write_mode).\n",
    "    :param write_mode: File mode ('w' for write/overwrite, 'a' for append).\n",
    "    :param write_header: Boolean flag to determine if the header row should be written.\n",
    "    :param limit: Optional row limit for testing.\n",
    "    :return: The number of rows saved in this run, or 0 on failure.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[START] Collecting {name}...\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # UniProt requires a User-Agent header with an email\n",
    "    headers = {\n",
    "        \"User-Agent\": f\"PythonScript/1.0 ({CONTACT_EMAIL})\"\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"format\": \"tsv\",     # TSV is safer to stream\n",
    "        \"fields\": \",\".join(fields)\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    max_retries = 5\n",
    "    initial_delay = 1\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # stream=True is essential for large datasets\n",
    "            with requests.get(API_URL, params=params, headers=headers, stream=True, timeout=120) as response:\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # iter_lines yields byte strings, we decode them\n",
    "                lines = response.iter_lines(decode_unicode=True)\n",
    "                \n",
    "                # Open file in the specified mode ('w' or 'a')\n",
    "                with open(filename, write_mode, newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    \n",
    "                    # --- Header Handling ---\n",
    "                    # The UniProt stream always starts with the header row.\n",
    "                    \n",
    "                    # Get the header line from the stream\n",
    "                    try:\n",
    "                        header_line = next(lines)\n",
    "                    except StopIteration:\n",
    "                        print(f\"[ERROR] No data found for {name} in this range.\")\n",
    "                        return 0 # No data\n",
    "                        \n",
    "                    if write_header and header_line:\n",
    "                        # Only write the header if requested (first chunk only)\n",
    "                        writer.writerow(header_line.split(\"\\t\"))\n",
    "                    # If write_header is False (append mode), we skip the line, but we must\n",
    "                    # consume it to get to the data rows. (Already done by next(lines))\n",
    "\n",
    "                    # --- Write Data ---\n",
    "                    for line in lines:\n",
    "                        if line:\n",
    "                            writer.writerow(line.split(\"\\t\"))\n",
    "                            count += 1\n",
    "                            \n",
    "                            # Progress indicator (every 50k rows for speed)\n",
    "                            if count % 50000 == 0:\n",
    "                                sys.stdout.write(f\"\\rRows saved for chunk: {count}\")\n",
    "                                sys.stdout.flush()\n",
    "                            \n",
    "                            if limit and count >= limit:\n",
    "                                print(f\"\\n[STOP] Reached limit of {limit} for this chunk.\")\n",
    "                                break\n",
    "                \n",
    "                print(f\"\\n[CHUNK DONE] Saved {count} rows for {name}.\")\n",
    "                return count # Success\n",
    "        \n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(f\"\\n[ERROR] HTTP Error on attempt {attempt + 1}/{max_retries}: {err}\")\n",
    "            if response.status_code == 403:\n",
    "                print(\"Hint: 403 usually means UniProt blocked the script. Check your CONTACT_EMAIL.\")\n",
    "                # Non-retriable error for 403, as backoff won't help\n",
    "                return 0\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\n[ERROR] Request failed on attempt {attempt + 1}/{max_retries}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[FATAL ERROR] Failed to download {name}: {e}\")\n",
    "            return 0 # Fatal error\n",
    "\n",
    "        # Exponential Backoff (for retriable errors)\n",
    "        if attempt < max_retries - 1:\n",
    "            delay = initial_delay * (2 ** attempt)\n",
    "            print(f\"Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            \n",
    "    print(f\"\\n[FAILED] Failed to complete download for {name} after {max_retries} attempts.\")\n",
    "    return 0\n",
    "\n",
    "# ==========================================\n",
    "# RUN\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    gamma_name = \"gammaproteobacteria\"\n",
    "    gamma_filename = f\"{gamma_name}.csv\"\n",
    "    \n",
    "    # Define the required modular length ranges\n",
    "    length_ranges = [\n",
    "        (100, 200), (201, 300), (301, 400), (401, 500), \n",
    "        (501, 600), (601, 700), (701, 800)\n",
    "    ]\n",
    "    \n",
    "    # Base query components for Gammaproteobacteria (Taxonomy ID 1236)\n",
    "    base_query_part = \"(taxonomy_id:1236) AND (fragment:false)\"\n",
    "    \n",
    "    print(f\"--- Starting Modular Download for {gamma_name} (Total {len(length_ranges)} blocks) ---\")\n",
    "    \n",
    "    # Flag to control writing the header only once\n",
    "    is_first_chunk = True\n",
    "    total_count = 0\n",
    "    \n",
    "    for start_len, end_len in length_ranges:\n",
    "        # Construct the full query for the specific length range\n",
    "        range_query_part = f\"(length:[{start_len} TO {end_len}])\"\n",
    "        full_query = f\"{base_query_part} AND {range_query_part}\"\n",
    "        \n",
    "        # Determine file write mode and header flag\n",
    "        # 1. First chunk uses 'w' (write/overwrite) to create the file and write the header.\n",
    "        # 2. Subsequent chunks use 'a' (append) and skip the header.\n",
    "        write_mode = \"w\" if is_first_chunk else \"a\"\n",
    "        write_header = is_first_chunk\n",
    "        \n",
    "        chunk_count = download_data(\n",
    "            name=f\"{gamma_name} (Length {start_len}-{end_len})\",\n",
    "            query=full_query,\n",
    "            filename=gamma_filename,\n",
    "            write_mode=write_mode,\n",
    "            write_header=write_header\n",
    "        )\n",
    "        \n",
    "        total_count += chunk_count\n",
    "            \n",
    "        # Set flag to False for all subsequent runs\n",
    "        is_first_chunk = False \n",
    "        \n",
    "    print(f\"\\n--- Modular Download Complete for {gamma_name}. ---\")\n",
    "    print(f\"Total Rows Saved to {gamma_filename}: {total_count}\")\n",
    "    \n",
    "    # NOTE: Other datasets defined in the original script have been omitted from the\n",
    "    # main execution block, as only the gammaproteobacteria modular download was requested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
